{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Step 1: Accept a search term from the user and download \n",
    "# the last 100 tweets from that search term \n",
    "#####################################################################\n",
    "\n",
    "# Install the python-twitter module. Unfortunately this module works only with Python 2 currently\n",
    "# and the Python 3 support is still under development. There are other modules that are similar though and\n",
    "# some are listed on the Twitter API documentation website \n",
    "# https://dev.twitter.com/overview/api/twitter-libraries\n",
    " \n",
    "\n",
    "# Otherwise, you can just go ahead and use !pip install python-twitter to install python-twitter for Python 2.  \n",
    "# This is a module that provides a python like interface to the Twitter API. The Twitter API is \n",
    "# fairly straightforward to use if you have used REST APIs before. A REST API provides information \n",
    "# in the form of a JSON which your application will have to parse once you get it. python-twitter \n",
    "# does this work for you and abstracts you from having to know the nitty-gritty of the Twitter API. In case the \n",
    "# module that you are using provides you a json output; you can use the json library to parse the tweets. This would\n",
    "# be an additional step that we have not shown you in our script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Tue Feb 27 12:03:48 +0000 2018\", \"default_profile\": true, \"default_profile_image\": true, \"friends_count\": 10, \"id\": 968456628990291968, \"id_str\": \"968456628990291968\", \"lang\": \"en\", \"name\": \"Prakash Das\", \"profile_background_color\": \"F5F8FA\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"prakshh18\", \"status\": {\"created_at\": \"Wed Mar 28 15:00:12 +0000 2018\", \"id\": 979010270889521152, \"id_str\": \"979010270889521152\", \"lang\": \"en\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"I found a #cloud #torrent that's awesome ;) @seedrcoil it's FREE just give it a try! https://t.co/DQ12oXgxLW\"}, \"statuses_count\": 1}\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "# Here we are importing the python-twitter module. (The library you import is called twitter, this \n",
    "# is a bit peculiar, but just remember that you will install python-twitter but in the import statement\n",
    "# import twitter)\n",
    "\n",
    "# The module provides an API object which has methods to get information from the Twitter API. To see \n",
    "# the complete documentation type pydoc twitter.Api at the command prompt in your terminal. This will \n",
    "# show you all the methods available, including those for fetching a user's statuses, a user's followers,\n",
    "# statuses for a particular search term etc \n",
    "# You can even post a status message to Twitter using this Api object but let's not go there right now :) \n",
    "\n",
    "# The Api object will need your Twitter API key/access credentials. Get these by registering your app \n",
    "# at https://apps:twitter.com/app/new \n",
    "\n",
    "api = twitter.Api(consumer_key='GyyG21OEUDKpILMCMSq06sVLk',\n",
    "                 consumer_secret='ayzwV5i1bW2rXMroGLHoJfdGlwBmZQJgN7TUXXZxZPD0UHS9zB',\n",
    "                 access_token_key='968456628990291968-8xIvRboXQ3QfDoNNUZJhuAOcqcJL4Yd',\n",
    "                 access_token_secret='3NZgWw1AlDrMqDhth4SBHsmiiSk5vyxpX6L5wrga6aEXQ')\n",
    "\n",
    "# To see if this worked, use the command below, it will print out a bunch of details about your user account\n",
    "# and that's how you know you're all set to use the API\n",
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! What are we searching for today?\"#Apple\"\n",
      "Great! We fetched 95 tweets with the term #Apple!!\n"
     ]
    }
   ],
   "source": [
    "# We're all set with our API \n",
    "# Now we'll set up a function to accept a search term and then fetch the tweets for that term \n",
    "\n",
    "def createTestData(search_string):\n",
    "    try:\n",
    "        tweets_fetched=api.GetSearch(search_string, count=100)\n",
    "        # This will return a list with twitter.Status objects. These have attributes for \n",
    "        # text, hashtags etc of the tweet that you are fetching. \n",
    "        # The full documentation again, you can see by typing pydoc twitter.Status at the \n",
    "        # command prompt of your terminal \n",
    "        print \"Great! We fetched \"+str(len(tweets_fetched))+\" tweets with the term \"+search_string+\"!!\"\n",
    "        # We will fetch only the text for each of the tweets, and since these don't have labels yet, \n",
    "        # we will keep the label empty \n",
    "        return [{\"text\":status.text,\"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print \"Sorry there was an error!\"\n",
    "        return None\n",
    "    \n",
    "search_string=input(\"Hi there! What are we searching for today?\")\n",
    "testData=createTestData(search_string)\n",
    "\n",
    "# Let's try that out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': None,\n",
       "  'text': u\"The world's most valuable company impressing investors with earnings...#Apple up ~7% afterhours as revenue &amp; profit\\u2026 https://t.co/vwZCyaOISt\"},\n",
       " {'label': None,\n",
       "  'text': u\"#Apple's wearable division now the size of a Fortune 300 company. #AppleWatch #Beats #AirPods\\u2026 https://t.co/kto6QAfC4z\"},\n",
       " {'label': None,\n",
       "  'text': u'\\u30a2\\u30c3\\u30d7\\u30eb\\u306e\\u30de\\u30a6\\u30b9\\u306e\\u52d5\\u304d\\u3092\\u826f\\u304f\\u3059\\u308b\\u30d1\\u30c3\\u30c9\\u300cPureShape\\u300d\\u767b\\u5834 https://t.co/hIPhUY82st #Apple https://t.co/JpSKKkAum5'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple feiert #Rekordzahlen f\\xfcr das erste #Quartal 2018\\nhttps://t.co/XjRoxANunA'},\n",
       " {'label': None,\n",
       "  'text': u'The low-cost #iPhoneSE2 is speculated to launch on June 4 during the #WWDC2018\\n^\\n#Apple #applenews #appleevent\\u2026 https://t.co/sArFVA8Svf'},\n",
       " {'label': None,\n",
       "  'text': u\"RT @jblefevre60: #Apple's #ARKit can visualize sound in space!\\nhttps://t.co/b4wAv4xY2C @WIRED #AR #VR\\n\\n@evankirstel @ipfconline1 @kalydeoo\\u2026\"},\n",
       " {'label': None,\n",
       "  'text': u'$AAPL #Apple Inc iPhone trends better than feared: Q2 revenue and EPS upside was driven by strong Services revenue\\u2026 https://t.co/lHPszRFAHt'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple: Umsatz- und Gewinnsteigerung im zweiten Quartal. Auf die Aktion\\xe4re wartet jetzt eine Sonderaussch\\xfcttung in\\u2026 https://t.co/uDtyC23Hwv'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple: Aktion\\xe4re erhalten weitere 100 Milliarden Dollar\\nhttps://t.co/GhVTq1OF05 https://t.co/AoxqlWw2gv'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet fetchedNow all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is\n",
      "[{u'message': u'Sorry, you are not authorized to see this status.', u'code': 179}]\n",
      "Tweet fetchedHilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\n",
      "Tweet fetched@RIM you made it too easy for me to switch to @Apple iPhone. See ya!\n",
      "[{u'message': u'No status found with that ID.', u'code': 144}]\n",
      "[{u'message': u'No status found with that ID.', u'code': 144}]\n",
      "Tweet fetchedThe 16 strangest things Siri has said so far. I am SOOO glad that @Apple gave Siri a sense of humor! http://t.co/TWAeUDBp via @HappyPlace\n",
      "Tweet fetchedGreat up close & personal event @Apple tonight in Regent St store!\n",
      "Tweet fetchedRT @cjwallace03: So apparently @apple put MB cap on your SMS with the new update. 25mb storage before it tells you your inbox is full. W ...\n",
      "Tweet fetchedRT @Jewelz2611 @mashable @apple, iphones r 2 expensive. Most went w/ htc/galaxy. No customer loyalty w/phone comp..\n",
      "Tweet fetched@mashable @apple, iphones r 2 expensive. Most went w/ htc/galaxy. No customer loyalty w/phone comp..\n",
      "[{u'message': u'No status found with that ID.', u'code': 144}]\n",
      "Tweet fetched@apple why my tunes no go on my iPhone? iPhone lonely without them. silly #iOS5\n",
      "Tweet fetched@apple needs to hurry up and release #iTunesMatch\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Step 2: Classify each of the 100 tweets as positive or negative\n",
    "################################################################\n",
    "\n",
    "# 2a. Download a corpus of tweets to use as training data\n",
    "# We'll use Niek Sanders's Tweet Sentiment Corpus. He has kindly provided 5000+ labelled tweets\n",
    "# WE can download a csv from his website with the tweets. But there is a catch, Twitter only allows \n",
    "# sharing of the tweet_id, so we'll have to fetch the text for each tweet id from the twitter API \n",
    "\n",
    "# We'll write a function that will read the csv we got from his website, for each tweet id in the \n",
    "# csv we'll download the tweet text and then write it back to another csv \n",
    "\n",
    "# 2a. Let's now write a separate function to download just 50 tweets for each label \n",
    "\n",
    "def createLimitedTrainingCorpus(corpusFile,tweetDataFile):\n",
    "    import csv\n",
    "    corpus=[]\n",
    "    with open(corpusFile,'rb') as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "            corpus.append({\"tweet_id\":row[2],\"label\":row[1],\"topic\":row[0]})\n",
    "    # We now have a list with a dictionary for each row in Sanders's csv\n",
    "    # Next let's iterate through that list and fetch the text for each tweet_id\n",
    "    \n",
    "    # If you try to download more than 180 tweets/15 min, Twitter will rate limit you. So, use a delay\n",
    "    # to avoid being rate limited. But this means it will take 10+ hours to download all 5000 tweets \n",
    "    # We'll show you the code to download all 5000 tweets, but for now, we'll work with a smaller corpus\n",
    "    # so we won't have to wait 10 hours to see our code run :) \n",
    "    \n",
    "    # To download the full corpus \n",
    "    \n",
    "    trainingData=[]\n",
    "    for label in [\"positive\",\"negative\"]:\n",
    "        i=1\n",
    "        for tweet in corpus:\n",
    "            if tweet[\"label\"]==label and i<=5:\n",
    "            # if tweet[\"label\"]==label and i<=50:\n",
    "                try:\n",
    "                    status=api.GetStatus(tweet[\"tweet_id\"])\n",
    "                    #Returns a twitter.Status object \n",
    "                    print \"Tweet fetched\" + status.text\n",
    "                    tweet[\"text\"]=status.text\n",
    "                    #tweet is a dictionary which already has tweet_id and label (positive/negative/neutral)\n",
    "                    # Add another attribute now, the tweet text \n",
    "                    trainingData.append(tweet)\n",
    "                    i=i+1\n",
    "                except Exception, e: \n",
    "                    print e\n",
    "                    \n",
    "    # Once the tweets are downloaded write them to a csv, so you won't have to wait 10 hours \n",
    "    # every time you run this code :) \n",
    "    with open(tweetDataFile,'wb') as csvfile:\n",
    "        linewriter=csv.writer(csvfile,delimiter=',',quotechar=\"\\\"\")\n",
    "        # We'll add a try catch block here so that we still get the training data even if the write \n",
    "        # fails \n",
    "        for tweet in trainingData:\n",
    "            try:\n",
    "                linewriter.writerow([tweet[\"tweet_id\"],tweet[\"text\"],tweet[\"label\"],tweet[\"topic\"]])\n",
    "            except Exception,e:\n",
    "                print e\n",
    "    return trainingData\n",
    "\n",
    "corpusFile=\"/home/prakash/twitter/twitter3/corpus.csv\"\n",
    "tweetDataFile=\"/home/prakash/twitter/twitter3/tweetDataFile.csv\"\n",
    "\n",
    "trainingData=createLimitedTrainingCorpus(corpusFile,tweetDataFile)\n",
    "# This will have saved our 150 tweets to a file and also returned a list with all the tweet data we \n",
    "# need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. A class to preprocess all the tweets, both test and training\n",
    "# We will use regular expressions and NLTK for preprocessing \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self,list_of_tweets):\n",
    "        # The list of tweets is a list of dictionaries which should have the keys, \"text\" and \"label\"\n",
    "        processedTweets=[]\n",
    "        # This list will be a list of tuples. Each tuple is a tweet which is a list of words and its label\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self,tweet):\n",
    "        # 1. Convert to lower case\n",
    "        tweet=tweet.lower()\n",
    "        # 2. Replace links with the word URL \n",
    "        tweet=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)     \n",
    "        # 3. Replace @username with \"AT_USER\"\n",
    "        tweet=re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "        # 4. Replace #word with word \n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        # You can do further cleanup as well if you like, replace \n",
    "        # repetitions of characters, for ex: change \"huuuuungry\" to \"hungry\"\n",
    "        # We'll leave that as an exercise for you on regular expressions\n",
    "        tweet=word_tokenize(tweet)\n",
    "        # This tokenizes the tweet into a list of words \n",
    "        # Let's now return this list minus any stopwords \n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "    \n",
    "tweetProcessor=PreProcessTweets()\n",
    "ppTrainingData=tweetProcessor.processTweets(trainingData)\n",
    "ppTestData=tweetProcessor.processTweets(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([u'get', u'swype', u'iphone', u'crack', u'iphone'], 'positive'),\n",
       " ([u'hilarious',\n",
       "   u'video',\n",
       "   u'guy',\n",
       "   u'duet',\n",
       "   u\"'s\",\n",
       "   u'siri',\n",
       "   u'pretty',\n",
       "   u'much',\n",
       "   u'sums',\n",
       "   u'love',\n",
       "   u'affair'],\n",
       "  'positive'),\n",
       " ([u'made', u'easy', u'switch', u'iphone', u'see', u'ya'], 'positive'),\n",
       " ([u'16',\n",
       "   u'strangest',\n",
       "   u'things',\n",
       "   u'siri',\n",
       "   u'said',\n",
       "   u'far',\n",
       "   u'sooo',\n",
       "   u'glad',\n",
       "   u'gave',\n",
       "   u'siri',\n",
       "   u'sense',\n",
       "   u'humor',\n",
       "   u'via'],\n",
       "  'positive'),\n",
       " ([u'great',\n",
       "   u'close',\n",
       "   u'personal',\n",
       "   u'event',\n",
       "   u'tonight',\n",
       "   u'regent',\n",
       "   u'st',\n",
       "   u'store'],\n",
       "  'positive')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppTrainingData[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2c. Extract features and train your classifier\n",
    "\n",
    "# We'll use two methods - Naive Bayes and Support Vector Machines \n",
    "\n",
    "import nltk \n",
    "# Naive Bayes Classifier - We'll use NLTK's built in classifier to perform the classification\n",
    "\n",
    "# First build a vocabulary \n",
    "def buildVocabulary(ppTrainingData):\n",
    "    all_words=[]\n",
    "    for (words,sentiment) in ppTrainingData:\n",
    "        all_words.extend(words)\n",
    "    # This will give us a list in which all the words in all the tweets are present\n",
    "    # These have to be de-duped. Each word occurs in this list as many times as it \n",
    "    # appears in the corpus \n",
    "    wordlist=nltk.FreqDist(all_words)\n",
    "    # This will create a dictionary with each word and its frequency\n",
    "    word_features=wordlist.keys()\n",
    "    # This will return the unique list of words in the corpus \n",
    "    return word_features\n",
    "\n",
    "# NLTK has an apply_features function that takes in a user-defined function to extract features \n",
    "# from training data. We want to define our extract features function to take each tweet in \n",
    "# The training data and represent it with the presence or absence of a word in the vocabulary \n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "        # This will give us a dictionary , with keys like 'contains word1' and 'contains word2'\n",
    "        # and values as True or False \n",
    "    return features \n",
    "\n",
    "# Now we can extract the features and train the classifier \n",
    "word_features = buildVocabulary(ppTrainingData)\n",
    "trainingFeatures=nltk.classify.apply_features(extract_features,ppTrainingData)\n",
    "# apply_features will take the extract_features function we defined above, and apply it it \n",
    "# each element of ppTrainingData. It automatically identifies that each of those elements \n",
    "# is actually a tuple , so it takes the first element of the tuple to be the text and \n",
    "# second element to be the label, and applies the function only on the text \n",
    "\n",
    "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
    "# We now have a classifier that has been trained using Naive Bayes\n",
    "\n",
    "# Support Vector Machines \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# We have to change the form of the data slightly. SKLearn has a CountVectorizer object. \n",
    "# It will take in documents and directly return a term-document matrix with the frequencies of \n",
    "# a word in the document. It builds the vocabulary by itself. We will give the trainingData \n",
    "# and the labels separately to the SVM classifier and not as tuples. \n",
    "# Another thing to take care of, if you built the Naive Bayes for more than 2 classes, \n",
    "# SVM can only classify into 2 classes - it is a binary classifier. \n",
    "\n",
    "svmTrainingData=[' '.join(tweet[0]) for tweet in ppTrainingData]\n",
    "# Creates sentences out of the lists of words \n",
    "\n",
    "vectorizer=CountVectorizer(min_df=1)\n",
    "X=vectorizer.fit_transform(svmTrainingData).toarray()\n",
    "# We now have a term document matrix \n",
    "vocabulary=vectorizer.get_feature_names()\n",
    "\n",
    "# Now for the twist we are adding to SVM. We'll use sentiwordnet to add some weights to these \n",
    "# features \n",
    "\n",
    "swn_weights=[]\n",
    "\n",
    "for word in vocabulary:\n",
    "    try:\n",
    "        # Put this code in a try block as all the words may not be there in sentiwordnet (esp. Proper\n",
    "        # nouns). Look for the synsets of that word in sentiwordnet \n",
    "        synset=list(swn.senti_synsets(word))\n",
    "        # use the first synset only to compute the score, as this represents the most common \n",
    "        # usage of that word \n",
    "        common_meaning =synset[0]\n",
    "        # If the pos_Score is greater, use that as the weight, if neg_score is greater, use -neg_score\n",
    "        # as the weight \n",
    "        if common_meaning.pos_score()>common_meaning.neg_score():\n",
    "            weight=common_meaning.pos_score()\n",
    "        elif common_meaning.pos_score()<common_meaning.neg_score():\n",
    "            weight=-common_meaning.neg_score()\n",
    "        else: \n",
    "            weight=0\n",
    "    except: \n",
    "        weight=0\n",
    "    swn_weights.append(weight)\n",
    "\n",
    "\n",
    "# Let's now multiply each array in our original matrix with these weights \n",
    "# Initialize a list\n",
    "\n",
    "swn_X=[]\n",
    "for row in X: \n",
    "    swn_X.append(np.multiply(row,np.array(swn_weights)))\n",
    "# Convert the list to a numpy array \n",
    "swn_X=np.vstack(swn_X)\n",
    "\n",
    "\n",
    "# We have our documents ready. Let's get the labels ready too. \n",
    "# Lets map positive to 1 and negative to 2 so that everything is nicely represented as arrays \n",
    "labels_to_array={\"positive\":1,\"negative\":2}\n",
    "labels=[labels_to_array[tweet[1]] for tweet in ppTrainingData]\n",
    "y=np.array(labels)\n",
    "\n",
    "# Let's now build our SVM classifier \n",
    "from sklearn.svm import SVC \n",
    "SVMClassifier=SVC()\n",
    "SVMClassifier.fit(swn_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2d: Run the classifier on the 100 downloaded tweets \n",
    "\n",
    "# First Naive Bayes \n",
    "NBResultLabels=[NBayesClassifier.classify(extract_features(tweet[0])) for tweet in ppTestData]\n",
    "\n",
    "# Now SVM \n",
    "SVMResultLabels=[]\n",
    "for tweet in ppTestData:\n",
    "    tweet_sentence=' '.join(tweet[0])\n",
    "    svmFeatures=np.multiply(vectorizer.transform([tweet_sentence]).toarray(),np.array(swn_weights))\n",
    "    SVMResultLabels.append(SVMClassifier.predict(svmFeatures)[0])\n",
    "    # predict() returns  a list of numpy arrays, get the first element of the first array \n",
    "    # there is only 1 element and array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Result Positive Sentiment91%\n",
      "SVM Result Negative Sentiment100%\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : GEt the majority vote and print the sentiment \n",
    "\n",
    "if NBResultLabels.count('positive')>NBResultLabels.count('negative'):\n",
    "    print \"NB Result Positive Sentiment\" + str(100*NBResultLabels.count('positive')/len(NBResultLabels))+\"%\"\n",
    "else: \n",
    "    print \"NB Result Negative Sentiment\" + str(100*NBResultLabels.count('negative')/len(NBResultLabels))+\"%\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if SVMResultLabels.count(1)>SVMResultLabels.count(2):\n",
    "    print \"SVM Result Positive Sentiment\" + str(100*SVMResultLabels.count(1)/len(SVMResultLabels))+\"%\"\n",
    "else: \n",
    "    print \"SVM Result Negative Sentiment\" + str(100*SVMResultLabels.count(2)/len(SVMResultLabels))+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': None,\n",
       "  'text': u\"The world's most valuable company impressing investors with earnings...#Apple up ~7% afterhours as revenue &amp; profit\\u2026 https://t.co/vwZCyaOISt\"},\n",
       " {'label': None,\n",
       "  'text': u\"#Apple's wearable division now the size of a Fortune 300 company. #AppleWatch #Beats #AirPods\\u2026 https://t.co/kto6QAfC4z\"},\n",
       " {'label': None,\n",
       "  'text': u'\\u30a2\\u30c3\\u30d7\\u30eb\\u306e\\u30de\\u30a6\\u30b9\\u306e\\u52d5\\u304d\\u3092\\u826f\\u304f\\u3059\\u308b\\u30d1\\u30c3\\u30c9\\u300cPureShape\\u300d\\u767b\\u5834 https://t.co/hIPhUY82st #Apple https://t.co/JpSKKkAum5'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple feiert #Rekordzahlen f\\xfcr das erste #Quartal 2018\\nhttps://t.co/XjRoxANunA'},\n",
       " {'label': None,\n",
       "  'text': u'The low-cost #iPhoneSE2 is speculated to launch on June 4 during the #WWDC2018\\n^\\n#Apple #applenews #appleevent\\u2026 https://t.co/sArFVA8Svf'},\n",
       " {'label': None,\n",
       "  'text': u\"RT @jblefevre60: #Apple's #ARKit can visualize sound in space!\\nhttps://t.co/b4wAv4xY2C @WIRED #AR #VR\\n\\n@evankirstel @ipfconline1 @kalydeoo\\u2026\"},\n",
       " {'label': None,\n",
       "  'text': u'$AAPL #Apple Inc iPhone trends better than feared: Q2 revenue and EPS upside was driven by strong Services revenue\\u2026 https://t.co/lHPszRFAHt'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple: Umsatz- und Gewinnsteigerung im zweiten Quartal. Auf die Aktion\\xe4re wartet jetzt eine Sonderaussch\\xfcttung in\\u2026 https://t.co/uDtyC23Hwv'},\n",
       " {'label': None,\n",
       "  'text': u'#Apple: Aktion\\xe4re erhalten weitere 100 Milliarden Dollar\\nhttps://t.co/GhVTq1OF05 https://t.co/AoxqlWw2gv'},\n",
       " {'label': None,\n",
       "  'text': u'20% OFF #sales #save #iWatch #apple #smartwatch @amazon @apple Apple Watch Series 1 42mm Smartwatch (Silver Aluminu\\u2026 https://t.co/YaXaiTuFsq'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBResultLabels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMResultLabels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like most of these tweets are actually neutral , And our SVM is classifying them as -ve,\n",
    "# But it classified the positive tweet correctly. \n",
    "\n",
    "# A few next steps possible here \n",
    "# Remove all neutral words according to sentiwordnet from the vocabulary. \n",
    "# Look at things like ALL CAPS , emoticons etc \n",
    "\n",
    "# GEt a corpus with more varied tweets (This one has only tech related tweets, so it works for our \n",
    "# search term (Apple) but might not for others. )\n",
    "\n",
    "# When required -\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
